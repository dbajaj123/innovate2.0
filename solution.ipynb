{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i4X5N0MheFOI"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import LSTM, Embedding, Dense, SimpleRNN\n",
        "from keras.models import Sequential\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "#importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "#mounting google drive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "In-KkiD_fiIo",
        "outputId": "de1c2ef7-8168-40e7-ddd8-66685b1ab6b9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('gdrive/My Drive/data.csv')\n",
        "#importing csv file\n",
        "\n",
        "raw_text = data['message']\n",
        "\n",
        "df_species = pd.get_dummies(data[\"species\"])\n",
        "#creating dummies of species for categorical classification\n",
        "\n",
        "df_tail = np.array(data[\"tail\"].apply(lambda x: (x==\"yes\")*1))\n",
        "df_fingers = np.array(data[\"fingers\"])\n",
        "#cleaning data\n",
        "\n",
        "y_train = np.array(df_species)*1\n"
      ],
      "metadata": {
        "id": "sSyFiKRQePhz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len=100\n",
        "max_words=10000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(raw_text)\n",
        "sequences = tokenizer.texts_to_sequences(raw_text)\n",
        "word_index = tokenizer.word_index\n",
        "print(len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "#Tokenizing raw text data for Model's use"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCnl1dw9e0ry",
        "outputId": "e18edc1e-653f-4b9c-91e4-0568b3cc9a88"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = data.T\n",
        "x_train = np.vstack((x_train, df_tail))\n",
        "x_train = np.vstack((x_train, df_fingers)).T\n",
        "\n",
        "#Adding Tail and Fingers Data to Tokenised Text Data"
      ],
      "metadata": {
        "id": "6otTFRzQfFho"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train, x_train.shape)\n",
        "print(y_train, y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQyjsun_fPYx",
        "outputId": "36e27b32-481c-4f3c-daa6-6394b699b00c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0   0   0 ...  91   0   4]\n",
            " [  0   0   0 ... 134   1   5]\n",
            " [  0   0   0 ...   6   1   5]\n",
            " ...\n",
            " [  0   0   0 ...  57   1   6]\n",
            " [  0   0   0 ... 635   1   2]\n",
            " [  0   0   0 ...   2   1   4]] (500, 102)\n",
            "[[1 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 1]\n",
            " [0 0 0 ... 0 0 1]\n",
            " ...\n",
            " [0 0 0 ... 0 0 1]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]] (500, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential()\n",
        "model.add(Embedding(10000, 1024))\n",
        "model.add(LSTM(1024))\n",
        "model.add(Dense(10, activation = 'sigmoid'))\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
        "#Creating a LSTM based Neural Network"
      ],
      "metadata": {
        "id": "HN7IiP4MfVdz"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit(x_train, y_train, epochs=15, batch_size=8, validation_split=0.02)\n",
        "\n",
        "#Training the model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxdxVwcpfZBN",
        "outputId": "b5bd3c7f-1cc4-44d8-9ae5-07d8f9e5c077"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - acc: 0.0842 - loss: 0.4247 - val_acc: 0.1000 - val_loss: 0.3772\n",
            "Epoch 2/15\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - acc: 0.1249 - loss: 0.3333 - val_acc: 0.0000e+00 - val_loss: 0.3365\n",
            "Epoch 3/15\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - acc: 0.1695 - loss: 0.3185 - val_acc: 0.3000 - val_loss: 0.3320\n",
            "Epoch 4/15\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - acc: 0.2002 - loss: 0.3011 - val_acc: 0.1000 - val_loss: 0.3643\n",
            "Epoch 5/15\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - acc: 0.2910 - loss: 0.2783 - val_acc: 0.4000 - val_loss: 0.2450\n",
            "Epoch 6/15\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - acc: 0.3349 - loss: 0.2683 - val_acc: 0.5000 - val_loss: 0.2258\n",
            "Epoch 7/15\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - acc: 0.4323 - loss: 0.2431 - val_acc: 0.4000 - val_loss: 0.2162\n",
            "Epoch 8/15\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - acc: 0.5955 - loss: 0.2058 - val_acc: 0.5000 - val_loss: 0.1670\n",
            "Epoch 9/15\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - acc: 0.6392 - loss: 0.1791 - val_acc: 1.0000 - val_loss: 0.1032\n",
            "Epoch 10/15\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - acc: 0.7071 - loss: 0.1598 - val_acc: 0.9000 - val_loss: 0.1079\n",
            "Epoch 11/15\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - acc: 0.8087 - loss: 0.1189 - val_acc: 0.9000 - val_loss: 0.0949\n",
            "Epoch 12/15\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - acc: 0.7886 - loss: 0.1065 - val_acc: 1.0000 - val_loss: 0.0482\n",
            "Epoch 13/15\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - acc: 0.8523 - loss: 0.0875 - val_acc: 0.9000 - val_loss: 0.0724\n",
            "Epoch 14/15\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - acc: 0.8338 - loss: 0.0794 - val_acc: 1.0000 - val_loss: 0.0569\n",
            "Epoch 15/15\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - acc: 0.8697 - loss: 0.0677 - val_acc: 1.0000 - val_loss: 0.0326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('gdrive/My Drive/test.csv')\n",
        "\n",
        "test_text = data['message']\n",
        "df_tail = np.array(data[\"tail\"].apply(lambda x: (x==\"yes\")*1))\n",
        "df_fingers = np.array(data[\"fingers\"])\n",
        "\n",
        "#Importing the test data and cleaning\n"
      ],
      "metadata": {
        "id": "UWjmONLWh1l6"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len=100\n",
        "max_words=10000\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(test_text)\n",
        "data = pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "#Tokenizing Raw test text\n",
        "\n"
      ],
      "metadata": {
        "id": "biwPIDy_h9ok"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = data.T\n",
        "x_test = np.vstack((x_test, df_tail))\n",
        "x_test = np.vstack((x_test, df_fingers)).T\n",
        "\n",
        "#Adding Tail and Fingers Data to Tokenized Text Data"
      ],
      "metadata": {
        "id": "27xhyG75iR55"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = model.predict(x_test)\n",
        "#Creating predctions based on test data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hkAzPtvh1tX",
        "outputId": "ac1943a9-ee4f-45d3-f871-e7516fd1cc79"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = np.argmax(prediction, axis=1)\n",
        "#Get highest probability class"
      ],
      "metadata": {
        "id": "uQ-wWSYaiyGG"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "species_headers = list(df_species)\n",
        "species_headers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsVy0dSClApr",
        "outputId": "478420bc-9532-4c05-bc75-d182fadaa329"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Aquari',\n",
              " 'Cybex',\n",
              " 'Emotivor',\n",
              " 'Faerix',\n",
              " 'Florian',\n",
              " 'Mythron',\n",
              " 'Nexoon',\n",
              " 'Quixnar',\n",
              " 'Sentire',\n",
              " 'Zorblax']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = []\n",
        "for key in output:\n",
        "  result.append(species_headers[key])\n",
        "result = pd.DataFrame(np.array(result))\n",
        "print(result[0])\n",
        "\n",
        "#Converting class indices to class names and saving in result array\n",
        "\n",
        "result.to_csv(\"result.csv\", index=False)\n",
        "#Exporting the predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MT34EJul7WG",
        "outputId": "050bb01f-c3ec-4436-cdc0-f761824f692f"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        Aquari\n",
            "1       Sentire\n",
            "2       Florian\n",
            "3        Faerix\n",
            "4        Nexoon\n",
            "         ...   \n",
            "294     Mythron\n",
            "295      Nexoon\n",
            "296     Mythron\n",
            "297     Quixnar\n",
            "298    Emotivor\n",
            "Name: 0, Length: 299, dtype: object\n"
          ]
        }
      ]
    }
  ]
}